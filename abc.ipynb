{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "abc.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anisha-g/distracted-driver-detection/blob/main/abc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5lvOMQDmJpp"
      },
      "source": [
        "# Distracted Driver Detection - Kaggle Competition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "91w6vG2rmJpq"
      },
      "source": [
        "#importing necessary library like numpy and pandas\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_fYLSQWmJpu"
      },
      "source": [
        "# importing necessary library \n",
        "import os    # to extract the data from system or if using kaggle then from directory\n",
        "from pathlib import Path      # to locate the path of file\n",
        "\n",
        "from keras.preprocessing import image   # to convert images to the array\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split    # for creating training and test data\n",
        "import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xfsUOCvmJpy"
      },
      "source": [
        "# This location is of kaggle dirctory where we can find image\n",
        "p=Path(\"../input/state-farm-distracted-driver-detection/imgs/train\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWmX_9WSmJp1"
      },
      "source": [
        "dirs=p.glob('*')  # accessing all the content images of above path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSwBzfSLmJp4"
      },
      "source": [
        "x=[]   # x will containt image array of each image\n",
        "y=[]    # y will contains label of images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQVqMzipmJp7"
      },
      "source": [
        "for i in dirs:\n",
        "    # as if i is converted into string then last char is digit can be assigne as number labels 0 to 9 for 10 labels\n",
        "    label=str(i).split('/')[-1]  \n",
        "    for img_path in i.glob('*.jpg'):    # going into each image of each labels for converting it into array\n",
        "        image_data=image.load_img(img_path,target_size=(100,100))  # loading the image into size of 100,100\n",
        "        \n",
        "        #converting each image into corresponding array\n",
        "        image_data_array=image.img_to_array(image_data)    \n",
        "        \n",
        "        # appending the above array into x\n",
        "        x.append(image_data_array)\n",
        "        \n",
        "        # appending the corresponding label into the y\n",
        "        y.append(label[-1])\n",
        "        \n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDzb4SJPmJp-"
      },
      "source": [
        "# converting list into numpy array for fast processing\n",
        "x=np.array(x)\n",
        "y=np.array(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qA2_rexCmJqE"
      },
      "source": [
        "# creating training and testing data with test size of 30% all all data\n",
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qt9O4JdGmJqI"
      },
      "source": [
        "# chekcing the shape of data\n",
        "x_train.shape,x_test.shape,y_train.shape,y_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhqKApCzmJqL"
      },
      "source": [
        "# encoding the y_train and y_test into one hot coding\n",
        "y_train=keras.utils.to_categorical(y_train)\n",
        "y_test=keras.utils.to_categorical(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_wHUox-mJqO"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pP3oOpfCmJqQ"
      },
      "source": [
        "y_train[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2U_cR66ZmJqU"
      },
      "source": [
        "# converting each array's value between 0 to 1\n",
        "x_train,x_test=x_train/255,x_test/255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrvsfSP5mJqX"
      },
      "source": [
        "x_train[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZVfQzmamJqZ"
      },
      "source": [
        "# Above all process is for date modification so that can be utilised for applyig machine learning algorith\n",
        "# below is for machine learning part applied through keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCS8OiCdmJqc"
      },
      "source": [
        "# importing models and layers from keras\n",
        "from keras import models,layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6ttGaVnmJqe"
      },
      "source": [
        "# using Sequential model of keras for applying predicting\n",
        "model=models.Sequential()\n",
        "\n",
        "# as we are working with images so convolution layer is being applied with 32 unit with filter of size 3,3 and padding same.\n",
        "# as, it is first layer so input size should be given as image is of size 100,100 and with three channel so input_shape is (100,100,3)\n",
        "model.add(layers.Conv2D(32, (3, 3),padding='same',input_shape=(100,100,3),activation='relu'))\n",
        "# maxpooling is applied for reducing the actual pixel of image       \n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "# dropout is applied for removing the redundancy\n",
        "model.add(layers.Dropout(0.5))\n",
        "\n",
        "# again convolutio layer is applied with 64 units of filte size(3,3) with padding same and activation function applied is relu\n",
        "model.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))\n",
        "\n",
        "# maxpooling is applied for reducing the actual pixel of image       \n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "# dropout is applied for removing the redundancy\n",
        "model.add(layers.Dropout(0.5))\n",
        "\n",
        "# again a convolution layer , maxpooling and dropout layer are applied.\n",
        "model.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))\n",
        "model.add(layers.MaxPooling2D((8, 8)))\n",
        "model.add(layers.Dropout(0.5))\n",
        "\n",
        "# as to give input to covolution layer is 2D and output is also 2D.\n",
        "# but input to dense neural network to be 1D, so converted into 1D using Flatten.\n",
        "model.add(layers.Flatten())\n",
        "# Flatten inpute is applied into Dense neural network of 300 units with activation function of relu\n",
        "model.add(layers.Dense(300,activation='relu'))\n",
        "# as final output should be one hot encodes of size 10, so final layer has 10 units and activation function applied is softmax for one hot encoding\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# Neural netwrok part is complete"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yh_NuaRYmJqg"
      },
      "source": [
        "# it is printing the summary of each layers and parameter in each layer.\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fxj-CEbpmJql"
      },
      "source": [
        "# now compiling the model with optimizer adam and loss function is categorical_crossentropy because of one-hot-encoding,and evaluaton of model will be done on accuracy\n",
        "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVKnG5iRmJqo"
      },
      "source": [
        "# fitting the model with training data, with epochs 10 helping in increasing accuracy, usng validationg data as testing data\n",
        "model.fit(x_train,y_train,epochs=10,validation_data=(x_test,y_test))\n",
        "# we can see that \n",
        "# accuracy is more than 94% which is better accuracy \n",
        "#but can be increased more by changing different epochs number and modify number of units in layers etc."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSO-6GwNmJqq"
      },
      "source": [
        "# predicting the accuracy on testing data\n",
        "model.evaluate(x_test,y_test)\n",
        "# we can see the testing accuracy is more than 98% which is more than better accuracy.\n",
        "# moreover accuracy on testing is more than training hence this model is not overfitting."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}